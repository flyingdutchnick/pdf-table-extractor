{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI and Custom Tool with LangChain to Extract Director Information\n",
    "\n",
    "This notebook contains code for creating a custom:\n",
    "\n",
    "Tool that \"reads\" annual reports\n",
    "Agent that uses this tool to:\n",
    "\\1. Identify the list of board members\n",
    "\\2. For each board member, extract their biography\n",
    "\\3. For each biography, save key information in a Dataframe\n",
    "\n",
    "In our example, the PDF is the 2021 proxy statement for Pfizer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Download dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install openai\n",
    "%pip install chromadb\n",
    "%pip install tiktoken\n",
    "%pip install pypdf\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load Pfizer's proxy statement. This may take 1-2 minutes since the PDF is 110 pages\n",
    "proxy_statement_pdf = \"https://s28.q4cdn.com/781576035/files/doc_financials/2022/sr/Proxy-Statement-2023.pdf\"\n",
    "# Create your PDF loader\n",
    "loader = PyPDFLoader(proxy_statement_pdf)\n",
    "# Load the PDF document\n",
    "documents = loader.load()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500)\n",
    "# Chunk the annual_report\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Save the annual report\n",
    "Using ChromaDB, save the annual report to a vector database. \n",
    "\n",
    "This will allow your custom Agent and Tool to later retrieve (use) the annual report for question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Create your custom Chain\n",
    "This Chain will be used by your custom Tool (defined next) to answer questions\n",
    "about the annual report that you previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.base import Chain\n",
    "from typing import Dict, List\n",
    "\n",
    "class AnnualReportChain(Chain):\n",
    "    chain: Chain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return list(self.chain.input_keys)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        # Queries the database to get the relevant documents for a given query\n",
    "        query = inputs.get(\"input_documents\", \"\")\n",
    "        docs = vectorstore.similarity_search(query, include_metadata=True)\n",
    "        output = chain.run(input_documents=docs, question=query)\n",
    "        return {'output': output}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Create your custom Tool\n",
    "This tool will use the Chain that you just created, under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nick-mbp/opt/anaconda3/lib/python3.9/site-packages/langchain/llms/openai.py:165: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/nick-mbp/opt/anaconda3/lib/python3.9/site-packages/langchain/llms/openai.py:676: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize your custom Chain\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "chain = load_qa_chain(llm)\n",
    "proxy_statement_chain = AnnualReportChain(chain=chain)\n",
    "\n",
    "# Initialize your custom Tool\n",
    "proxy_statement_tool = Tool(\n",
    "    name=\"Proxy Statement\",\n",
    "    func=proxy_statement_chain.run,\n",
    "    description=\"\"\"\n",
    "    Useful for when you need to answer questions about a company's board of directors. \n",
    "    This tool can help you extract data points like the names of director nominees, their past experience,\n",
    "    skills and current positions held.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Create your custom Agent\n",
    "This Agent uses your custom tool(s) to get things done.\n",
    "\n",
    "For our example, the Agent is given 1 tool (`proxy_statement_tool` from above) and answers questions about annual reports!\n",
    "\n",
    "The code here is heavily borrowed from [this wonderful GitHub repository](https://github.com/mpaepper/llm_agents), which is created by [Marc PÃ¤pper](https://twitter.com/mpaepper).\n",
    "\n",
    "Marc wrote an [excellent blog post](https://www.paepper.com/blog/posts/intelligent-agents-guided-by-llms/) that explains how Agents work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Tuple\n",
    "\n",
    "class Agent(BaseModel):\n",
    "    # The large language model that the Agent will use to decide the action to take\n",
    "    llm: BaseModel\n",
    "    # The prompt that the language model will use and append previous responses to\n",
    "    prompt: str\n",
    "    # The list of tools that the Agent can use\n",
    "    tools: List[Tool]\n",
    "    # Adjust this so that the Agent does not loop infinitely\n",
    "    max_loops: int = 20\n",
    "    # The stop pattern is used, so the LLM does not hallucinate until the end\n",
    "    stop_pattern: List[str]\n",
    "\n",
    "    @property\n",
    "    def tool_by_names(self) -> Dict[str, Tool]:\n",
    "        return {tool.name: tool for tool in self.tools}\n",
    "\n",
    "    def run(self, question: str):\n",
    "        name_to_tool_map = {tool.name: tool for tool in self.tools}\n",
    "        previous_responses = []\n",
    "        num_loops = 0\n",
    "        while num_loops < self.max_loops:\n",
    "            num_loops += 1\n",
    "            curr_prompt = prompt.format(previous_responses=('\\n'.join(previous_responses)))\n",
    "            output, tool, tool_input = self._get_next_action(curr_prompt)\n",
    "            if tool == 'Final Answer':\n",
    "                return tool_input\n",
    "            tool_result = name_to_tool_map[tool].run(tool_input)\n",
    "            output += f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n{THOUGHT_TOKEN}\"\n",
    "            print(output)\n",
    "            previous_responses.append(output)\n",
    "\n",
    "    def _get_next_action(self, prompt: str) -> Tuple[str, str, str]:\n",
    "        # Use the LLM to generate the Agent's next action\n",
    "        result = self.llm.generate([prompt], stop=self.stop_pattern)\n",
    "\n",
    "        # List of the things generated. This is List[List[]] because each input could have multiple generations.\n",
    "        generations = result.generations\n",
    "\n",
    "        # Grab the first text generation, as this will likely be the best result\n",
    "        output = generations[0][0].text\n",
    "\n",
    "        # Parse the result\n",
    "        tool, tool_input = self._get_tool_and_input(output)\n",
    "        return output, tool, tool_input\n",
    "\n",
    "    def _get_tool_and_input(self, generated: str) -> Tuple[str, str]:\n",
    "        if FINAL_ANSWER_TOKEN in generated:\n",
    "            return \"Final Answer\", generated.split(FINAL_ANSWER_TOKEN)[-1].strip()\n",
    "\n",
    "        regex = r\"Action: [\\[]?(.*?)[\\]]?[\\n]*Action Input:[\\s]*(.*)\"\n",
    "        match = re.search(regex, generated, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Output of LLM is not parsable for next tool use: `{generated}`\")\n",
    "        tool = match.group(1).strip()\n",
    "        tool_input = match.group(2)\n",
    "        return tool, tool_input.strip(\" \").strip('\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Create your Prompt template\n",
    "This prompt will be fed into the Agent's large language model (LLM).  \n",
    "\n",
    "As it \"reasons\" and answers your query, the Agent will update this prompt by appending the previous response (context) to the prompt to maintain context of its overall \"chain of thought\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_ANSWER_TOKEN = \"Final Answer:\"\n",
    "OBSERVATION_TOKEN = \"Observation:\"\n",
    "THOUGHT_TOKEN = \"Thought:\"\n",
    "PROMPT_TEMPLATE = \"\"\"Answer the question as best as you can using the following tools: \n",
    "\n",
    "{tool_description}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: comment on what you want to do next\n",
    "Action: the action to take, exactly one element of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action, including extracting any relevant text that answers the question\n",
    "... (this Thought/Action/Action Input/Observation repeats N times, use it until you are sure of the answer)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: your final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {question}\n",
    "Thought: {previous_responses}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Run your custom Agent\n",
    "You can update the `question` variable to ask your Agent to answer questions about the PDF that you previously loaded!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to find the list of director nominees for a company.\n",
      "Action: Proxy Statement\n",
      "Action Input: Search for the most recent proxy statement for the company.\n",
      "Observation: The most recent proxy statement for Pfizer can be found on their website at https://investors.pfizer.com/financials/proxy-materials/default.aspx.\n",
      "Thought:\n",
      "I need to search for the list of director nominees in the proxy statement.\n",
      "Action: Proxy Statement\n",
      "Action Input: Open the most recent proxy statement for Pfizer.\n",
      "Observation: The most recent proxy statement for Pfizer is the 2023 Proxy Statement, which is available on their website at https://investors.pfizer.com/Investors/Corporate-Governance/Contact-Our-Directors/default.aspx.\n",
      "Thought:\n",
      "I need to search for the section of the proxy statement that lists the director nominees.\n",
      "Action: Proxy Statement\n",
      "Action Input: Navigate to the section titled \"Election of Directors\" in the 2023 Proxy Statement for Pfizer.\n",
      "Observation: The section titled \"Election of Directors\" can be found on page 4 of the 2023 Proxy Statement for Pfizer.\n",
      "Thought:\n",
      "I need to extract the names of the director nominees from the \"Election of Directors\" section.\n",
      "Action: Proxy Statement\n",
      "Action Input: Read through the \"Election of Directors\" section of the 2023 Proxy Statement for Pfizer.\n",
      "Observation: The \"Election of Directors\" section of the 2023 Proxy Statement for Pfizer includes information on the criteria for board membership, the selection process for director nominees, director independence, and the list of 2023 director nominees. It also explains that the proxy committee appointed by the Board of Directors will vote on behalf of shareholders who do not indicate their voting preferences, and that representatives of Pfizer's transfer agent, Computershare, will tabulate the votes and act as inspectors of election. Shareholders can revoke their proxy before it is exercised by giving written notice to the Corporate Secretary, delivering a valid, later-dated proxy, or voting at the virtual Annual Meeting. Pfizer will pay for the cost of soliciting proxies, and the Board is not aware of any other matters expected to come before the 2023 Annual Meeting.\n",
      "Thought:\n",
      "The list of director nominees for Pfizer can be found in the \"Election of Directors\" section of the 2023 Proxy Statement, which includes information on the criteria for board membership, the selection process for director nominees, director independence, and the list of 2023 director nominees.\n"
     ]
    }
   ],
   "source": [
    "# The tool(s) that your Agent will use\n",
    "tools = [proxy_statement_tool]\n",
    "\n",
    "# The question that you will ask your Agent\n",
    "question = \"Who are the director nominees?\"\n",
    "\n",
    "# The prompt that your Agent will use and update as it is \"reasoning\"\n",
    "prompt = PROMPT_TEMPLATE.format(\n",
    "  tool_description=\"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools]),\n",
    "  tool_names=\", \".join([tool.name for tool in tools]),\n",
    "  question=question,\n",
    "  previous_responses='{previous_responses}',\n",
    ")\n",
    "\n",
    "# The LLM that your Agent will use\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize your Agent\n",
    "agent = Agent(\n",
    "  llm=llm, \n",
    "  tools=tools, \n",
    "  prompt=prompt, \n",
    "  stop_pattern=[f'\\n{OBSERVATION_TOKEN}', f'\\n\\t{OBSERVATION_TOKEN}'],\n",
    ")\n",
    "\n",
    "# Run the Agent!\n",
    "result = agent.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
